{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción Big data analisis con Spark\n",
    "\n",
    "¿Qúe es Big Data?\n",
    "\n",
    "Big data es un término utilizado para referirse al estudio y las aplicaciones de conjuntos de datos que son demasiado complejos para el software tradicional de procesamiento de datos.\n",
    "\n",
    "Las 3 V de Big Data\n",
    "\n",
    "- Volumen: Tamaño de los datos\n",
    "- Variedad: Diferentes formatos\n",
    "- Velocidad: Velocidad de procesamiento de datos\n",
    "\n",
    "### Terminos\n",
    "\n",
    "- Computación en clúster: Colección de recursos de varias máquinas \n",
    "\n",
    "- Computación paralela: Computación simultánea\n",
    "\n",
    "- Computación distribuida: Colección de nodos (computadoras en red) que se ejecutan en paralelo \n",
    "\n",
    "- Procesamiento por lotes: Romper el trabajo en piezas pequeñas y ejecutarlas en máquinas individuales\n",
    "\n",
    "- Procesamiento en tiempo real: Procesamiento inmediato\n",
    "\n",
    "### Sistemas que procesan Big Data\n",
    "\n",
    "- Hadoop/MapReduce: Escalable tolerante a fallas escrito en Java, procesamiento por lotes.\n",
    "\n",
    "- Apache Spark: Sistema informático de clúster de uso general y veloz, Procesamiento de datos por lotes y en tiempo real.\n",
    "\n",
    "### Características del marco Apache Spark \n",
    "\n",
    "- Marco de cómputo de clúster distribuido \n",
    "\n",
    "- Cálculos eficientes en memoria para grandes conjuntos de datos\n",
    "\n",
    "- Marco de procesamiento de datos rápido como el rayo\n",
    "\n",
    "- Brinda soporte para Java, Scala, Python, R y SQL\n",
    "\n",
    "### Componentes\n",
    "\n",
    "- Spark SQL\n",
    "\n",
    "- MLlib\n",
    "\n",
    "- GraphX\n",
    "\n",
    "- Spark Streaming\n",
    "\n",
    "- RDD API Apache Spark Core\n",
    "\n",
    "### Modos de despliegue Spark \n",
    "\n",
    "- Modo local: máquina individual, como su computadora portátil \n",
    "\n",
    "    - Modelo local conveniente para pruebas, depuración y demostración \n",
    "\n",
    "\n",
    "- Modo de clúster: conjunto de máquinas predefinidas \n",
    "\n",
    "    - Bueno para producción Workow: Local -> clusters\n",
    "    \n",
    "    - No es necesario cambiar el código\n",
    "    \n",
    "### La descripción general de PySpark\n",
    "\n",
    "- Apache Spark está escrita en Scala\n",
    "\n",
    "- Para admitir Python con Spark, Apache Spark Community lanzó PySpark\n",
    "\n",
    "- Velocidad y potencia de cálculo similares a las Scala \n",
    "\n",
    "- PySpark API similares a Pandas y Scikit-learn\n",
    "\n",
    "### ¿Que es Spark Shell?\n",
    "\n",
    "- Entorno interactivo\n",
    "\n",
    "- Útil para la creación rápida de prototipos interactivos\n",
    "\n",
    "- Los shells de Spark permiten interactuar con los datos en el disco o en la memoria Tres shells diferentes de Spark: Spark-shell para Scala PySpark-shell para Python  SparkR para R\n",
    "\n",
    "### PySpark shell \n",
    "\n",
    "- PySpark shell es la herramienta de línea de comandos basada en Python.\n",
    "\n",
    "- PySpark shell permite a los científicos de datos interactuar con estructuras de datos Spark.\n",
    "\n",
    "- PySpark shell admite la conexión a un clúster.\n",
    "\n",
    "### Comprensión de SparkContext\n",
    "\n",
    "- SparkContext es un punto de entrada al mundo de Spark\n",
    "\n",
    "- Un punto de entrada es una forma de conectarse al clúster Spark\n",
    "\n",
    "- Un punto de entrada es como una llave de la casa \n",
    "\n",
    "- PySpark tiene un SparkContext predeterminado llamado sc\n",
    "\n",
    "### Comandos para inspeccionar SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext('local', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is 3.1.2\n",
      "The Python version of Spark Context in the PySpark shell is 3.9\n",
      "The master of Spark Context in the PySpark shell is local\n"
     ]
    }
   ],
   "source": [
    "#SparContext - punto de entreda para interectuar con Spark\n",
    "\n",
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cargar datos PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5]) \n",
    "\n",
    "rdd2 = sc.textFile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Uso de funciones Lambda, map(), filter()\n",
    "\n",
    "- Son altamente usadas en PySpark\n",
    "\n",
    "- Facilitan la escritura y eficiencia del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "The squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "squared_list_lambda = list(map(lambda x: x **2, my_list))\n",
    "\n",
    "print(\"The squared numbers are\", squared_list_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is: [10, 21, 31, 41, 51, 60, 72, 80, 93, 101]\n",
      "Numbers divisible by 10 are: [10, 60, 80]\n"
     ]
    }
   ],
   "source": [
    "my_list2 = [10,21,31,41,51,60,72,80,93,101]\n",
    "\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programando con Pyspark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyPark SQL y Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning con PySpark MLlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
