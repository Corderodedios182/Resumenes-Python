{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de Apache Spark: Funciones avanzadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook aprenderemos algunas funciones avanzadas para optimizar el rendimiento de Spark, para imputar valores faltantes o a crear funciones definidas por el usuario (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea la sesión de SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (None, None, None, 7500),\n",
    "    (9, \"III\", None, 4500),\n",
    "    (10, None, \"dept5\", 2500)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) \n",
    "\n",
    "# Create Temp Tables\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")\n",
    "\n",
    "# Save as HIVE tables.\n",
    "df.write.saveAsTable(\"hive_empdf\", mode = \"overwrite\")\n",
    "deptdf.write.saveAsTable(\"hive_deptdf\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadCast Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tamaño de la tabla de difusión es de 10 MB. Sin embargo, podemos cambiar el umbral hasta 8GB según la documentación oficial de Spark 2.3.\n",
    "\n",
    "* Podemos verificar el tamaño de la tabla de transmisión de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default size of broadcast table is 50.0 MB.\n"
     ]
    }
   ],
   "source": [
    "size = int(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")) / (1024 * 1024)\n",
    "print(\"Default size of broadcast table is {0} MB.\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos establecer el tamaño de la tabla de transmisión para que diga 50 MB de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considere que necesitamos unir 2 Dataframes.\n",
    "# small_df: DataFrame pequeño que puede caber en la memoria y es más pequeño que el umbral especificado.\n",
    "# big_df: DataFrame grande que debe unirse con DataFrame pequeño.\n",
    "\n",
    "join_df = big_df.join(broadcast(small_df), big_df[\"id\"] == small_df[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento en caché\n",
    "Podemos usar la función de caché / persistencia para mantener el marco de datos en la memoria. Puede mejorar significativamente el rendimiento de su aplicación Spark si almacenamos en caché los datos que necesitamos usar con mucha frecuencia en nuestra aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "df.count()\n",
    "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando usamos la función de caché, usará el nivel de almacenamiento como Memory_Only hasta Spark 2.0.2. Desde Spark 2.1.x es Memory_and_DISK.\n",
    "\n",
    "Sin embargo, si necesitamos especificar los distintos niveles de almacenamiento disponibles, podemos usar el método persist( ). Por ejemplo, si necesitamos mantener los datos solo en la memoria, podemos usar el siguiente fragmento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    }
   ],
   "source": [
    "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
    "deptdf.count()\n",
    "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No persistir\n",
    "También es importante eliminar la memoria caché de los datos cuando ya no sean necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, dept: string, salary: bigint]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Expresiones SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos usar la expresión SQL para la manipulación de datos. Tenemos la función **expr** y también una variante de un método de selección como **selectExpr** para la evaluación de expresiones SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|null|null| null|  7500| high_salary|\n",
      "|   9| III| null|  4500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Intentemos categorizar el salario en Bajo, Medio y Alto según la categorización a continuación.\n",
    "\n",
    "# 0-2000: salario_bajo\n",
    "# 2001 - 5000: mid_salary\n",
    "#> 5001: high_salary\n",
    "\n",
    "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
    "               else case when salary > 2000 then 'mid_salary'\n",
    "                    else case when salary > 0 then 'low_salary'\n",
    "                         else 'invalid_salary'\n",
    "                              end\n",
    "                         end\n",
    "                end as salary_level\"\"\"\n",
    "\n",
    "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la función selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|null|null| null|  7500| high_salary|\n",
      "|   9| III| null|  4500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"*\", cond)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones definidas por el usuario (UDF)\n",
    "A menudo necesitamos escribir la función en función de nuestro requisito muy específico. Aquí podemos aprovechar las udfs. Podemos escribir nuestras propias funciones en un lenguaje como python y registrar la función como udf, luego podemos usar la función para operaciones de DataFrame.\n",
    "\n",
    "* Función de Python para encontrar el nivel_salario para un salario dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detSalary_Level(sal):\n",
    "    level = None\n",
    "\n",
    "    if(sal > 5000):\n",
    "        level = 'high_salary'\n",
    "    elif(sal > 2000):\n",
    "        level = 'mid_salary'\n",
    "    elif(sal > 0):\n",
    "        level = 'low_salary'\n",
    "    else:\n",
    "        level = 'invalid_salary'\n",
    "    return level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego registre la función \"detSalary_Level\" como UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_level = udf(detSalary_Level, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aplicar función para determinar el salario_level para un salario dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|null|null| null|  7500| high_salary|\n",
      "|   9| III| null|  4500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando con valores NULL\n",
    "\n",
    "Los valores NULL siempre son difíciles de manejar independientemente del Framework o lenguaje que usemos. Aquí en Spark tenemos pocas funciones específicas para lidiar con valores NULL.\n",
    "\n",
    "- **es nulo()**\n",
    "\n",
    "Esta función nos ayudará a encontrar los valores nulos para cualquier columna dada. Por ejemplo si necesitamos encontrar las columnas donde las columnas id contienen los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+\n",
      "|  id|name|dept|salary|\n",
      "+----+----+----+------+\n",
      "|null|null|null|  7500|\n",
      "|   9| III|null|  4500|\n",
      "+----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **No es nulo()**\n",
    "\n",
    "Esta función funciona de manera opuesta a la función isNull () y devolverá todos los valores no nulos para una función en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "| 10|null|dept5|  2500|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNotNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **fillna ()**\n",
    "\n",
    "Esta función nos ayudará a reemplazar los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+------+\n",
      "|  id|name|   dept|salary|\n",
      "+----+----+-------+------+\n",
      "|   1| AAA|  dept1|  1000|\n",
      "|   2| BBB|  dept1|  1100|\n",
      "|   3| CCC|  dept1|  3000|\n",
      "|   4| DDD|  dept1|  1500|\n",
      "|   5| EEE|  dept2|  8000|\n",
      "|   6| FFF|  dept2|  7200|\n",
      "|   7| GGG|  dept3|  7100|\n",
      "|null|null|INVALID|  7500|\n",
      "|   9| III|INVALID|  4500|\n",
      "|  10|null|  dept5|  2500|\n",
      "+----+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace -1 where the salary is null.\n",
    "newdf = df.fillna(\"INVALID\", [\"dept\"])\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **dropna ()**\n",
    "\n",
    "Esta función nos ayudará a eliminar las filas con valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows which contains any null values.\n",
    "newdf = df.dropna()\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina todas las filas que contienen todos los valores nulos.\n",
    "newdf = df.dropna(how = \"all\")\n",
    "newdf.show()\n",
    "\n",
    "# Nota: valor predeterminado de \"cómo\" param es \"any\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "| 10|null|dept5|  2500|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows where columns : dept is null.\n",
    "newdf = df.dropna(subset = \"dept\")\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "\n",
    "El particionamiento es un aspecto muy importante para controlar el paralelismo de la aplicación Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consultar número de particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Incrementar el número de particiones. Por ejemplo Aumentar las particiones a 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.repartition(6)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota: se trata de operaciones costosas, ya que requiere la mezcla de datos entre los trabajadores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Disminuir el número de particiones. Por ejemplo disminuir las particiones a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.coalesce(2)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* De forma predeterminada, el número de particiones para Spark SQL es 200.\n",
    "* Pero también podemos establecer el número de particiones en el nivel de aplicación Spark. Por ejemplo establecido en 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Partitions : 500\n"
     ]
    }
   ],
   "source": [
    "# Set number of partitions as Spark Application.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "# Check the number of patitions.\n",
    "num_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(\"No of Partitions : {0}\".format(num_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catálogo de APIs\n",
    "\n",
    "Spark Catalog es una API orientada al usuario, a la que puede acceder mediante SparkSession.catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **listDatabases ()**\n",
    "\n",
    "Devolverá todas las bases de datos junto con su ubicación en el sistema de archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/jovyan/work/spark-warehouse')]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **listTables ()**\n",
    "\n",
    "Devolverá todas las tablas para una base de datos determinada junto con información como el tipo de tabla (externa / administrada) y si una tabla en particular es temporal o permanente.\n",
    "Esto incluye todas las vistas temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='hive_deptdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='hive_empdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='deptdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='empdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **listColumns ()**\n",
    "\n",
    "Devolverá todas las columnas de una tabla en particular en DataBase. Además, devolverá el tipo de datos, si la columna se usa en particiones o agrupaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='id', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='dept', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"hive_empdf\", \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **listFunctions()**\n",
    "\n",
    "Devolverá todas las funciones disponibles en Spark Session junto con la información si es temporal o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
       " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
       " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
       " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
       " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
       " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
       " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
       " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
       " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
       " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
       " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
       " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
       " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
       " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
       " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
       " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
       " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
       " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
       " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
       " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
       " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
       " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
       " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
       " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
       " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
       " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
       " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
       " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
       " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
       " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
       " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
       " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
       " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
       " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
       " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
       " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
       " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
       " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
       " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
       " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
       " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
       " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
       " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
       " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
       " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
       " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
       " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
       " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
       " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
       " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
       " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
       " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
       " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
       " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
       " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
       " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
       " Function(name='cube', description=None, className='org.apache.spark.sql.catalyst.expressions.Cube', isTemporary=True),\n",
       " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
       " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
       " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
       " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
       " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
       " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
       " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
       " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
       " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
       " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
       " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
       " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
       " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
       " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
       " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
       " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
       " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
       " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
       " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
       " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
       " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
       " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
       " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
       " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
       " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
       " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
       " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
       " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
       " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
       " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
       " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
       " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
       " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
       " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
       " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
       " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
       " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
       " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
       " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
       " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
       " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
       " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
       " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
       " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
       " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
       " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
       " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
       " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
       " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
       " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
       " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
       " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
       " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
       " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
       " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
       " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
       " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
       " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
       " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
       " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
       " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
       " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
       " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
       " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
       " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
       " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
       " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
       " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
       " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
       " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
       " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
       " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
       " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
       " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
       " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
       " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
       " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
       " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
       " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
       " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
       " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
       " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
       " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
       " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
       " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
       " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
       " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
       " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
       " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
       " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
       " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
       " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
       " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
       " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
       " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
       " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
       " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
       " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
       " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
       " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='rollup', description=None, className='org.apache.spark.sql.catalyst.expressions.Rollup', isTemporary=True),\n",
       " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
       " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
       " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
       " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
       " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
       " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
       " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
       " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
       " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
       " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
       " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
       " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
       " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
       " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
       " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
       " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
       " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
       " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
       " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
       " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
       " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
       " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
       " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
       " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
       " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
       " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
       " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.NamedStruct', isTemporary=True),\n",
       " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
       " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
       " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
       " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
       " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
       " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
       " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
       " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
       " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
       " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
       " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
       " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
       " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
       " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
       " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
       " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
       " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
       " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
       " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
       " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
       " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
       " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
       " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
       " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
       " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
       " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
       " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
       " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
       " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
       " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
       " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
       " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
       " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listFunctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **currentDatabase ()**\n",
    "\n",
    "Obtenga la base de datos actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **setCurrentDatabase ()**\n",
    "\n",
    "Establecer la base de datos actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.catalog.setCurrentDatabase(<DB_Name>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **cacheTable ()**\n",
    "\n",
    "almacenar en caché una tabla en particular.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **isCached()**\n",
    "\n",
    "Compruebe si la tabla está almacenada en caché o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.isCached(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **uncacheTable()**\n",
    "\n",
    "Des-cachear de una tabla en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify uncached table. Now you will see that it will return \"False\" which means table is not cached.\n",
    "spark.catalog.isCached(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **clearCache()**\n",
    "\n",
    "Des-cachear toda la tabla en la sesión de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
