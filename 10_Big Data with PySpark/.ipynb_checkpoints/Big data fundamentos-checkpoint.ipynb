{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción Big data analisis con Spark\n",
    "\n",
    "¿Qúe es Big Data?\n",
    "\n",
    "Big data es un término utilizado para referirse al estudio y las aplicaciones de conjuntos de datos que son demasiado complejos para el software tradicional de procesamiento de datos.\n",
    "\n",
    "Las 3 V de Big Data\n",
    "\n",
    "- Volumen: Tamaño de los datos\n",
    "- Variedad: Diferentes formatos\n",
    "- Velocidad: Velocidad de procesamiento de datos\n",
    "\n",
    "### Terminos\n",
    "\n",
    "- Computación en clúster: Colección de recursos de varias máquinas \n",
    "\n",
    "- Computación paralela: Computación simultánea\n",
    "\n",
    "- Computación distribuida: Colección de nodos (computadoras en red) que se ejecutan en paralelo \n",
    "\n",
    "- Procesamiento por lotes: Romper el trabajo en piezas pequeñas y ejecutarlas en máquinas individuales\n",
    "\n",
    "- Procesamiento en tiempo real: Procesamiento inmediato\n",
    "\n",
    "### Sistemas que procesan Big Data\n",
    "\n",
    "- Hadoop/MapReduce: Escalable tolerante a fallas escrito en Java, procesamiento por lotes.\n",
    "\n",
    "- Apache Spark: Sistema informático de clúster de uso general y veloz, Procesamiento de datos por lotes y en tiempo real.\n",
    "\n",
    "### Características del marco Apache Spark \n",
    "\n",
    "- Marco de cómputo de clúster distribuido \n",
    "\n",
    "- Cálculos eficientes en memoria para grandes conjuntos de datos\n",
    "\n",
    "- Marco de procesamiento de datos rápido como el rayo\n",
    "\n",
    "- Brinda soporte para Java, Scala, Python, R y SQL\n",
    "\n",
    "### Componentes\n",
    "\n",
    "- Spark SQL\n",
    "\n",
    "- MLlib\n",
    "\n",
    "- GraphX\n",
    "\n",
    "- Spark Streaming\n",
    "\n",
    "- RDD API Apache Spark Core\n",
    "\n",
    "### Modos de despliegue Spark \n",
    "\n",
    "- Modo local: máquina individual, como su computadora portátil \n",
    "\n",
    "    - Modelo local conveniente para pruebas, depuración y demostración \n",
    "\n",
    "\n",
    "- Modo de clúster: conjunto de máquinas predefinidas \n",
    "\n",
    "    - Bueno para producción Workow: Local -> clusters\n",
    "    \n",
    "    - No es necesario cambiar el código\n",
    "    \n",
    "### La descripción general de PySpark\n",
    "\n",
    "- Apache Spark está escrita en Scala\n",
    "\n",
    "- Para admitir Python con Spark, Apache Spark Community lanzó PySpark\n",
    "\n",
    "- Velocidad y potencia de cálculo similares a las Scala \n",
    "\n",
    "- PySpark API similares a Pandas y Scikit-learn\n",
    "\n",
    "### ¿Que es Spark Shell?\n",
    "\n",
    "- Entorno interactivo\n",
    "\n",
    "- Útil para la creación rápida de prototipos interactivos\n",
    "\n",
    "- Los shells de Spark permiten interactuar con los datos en el disco o en la memoria Tres shells diferentes de Spark: Spark-shell para Scala PySpark-shell para Python  SparkR para R\n",
    "\n",
    "### PySpark shell \n",
    "\n",
    "- PySpark shell es la herramienta de línea de comandos basada en Python.\n",
    "\n",
    "- PySpark shell permite a los científicos de datos interactuar con estructuras de datos Spark.\n",
    "\n",
    "- PySpark shell admite la conexión a un clúster.\n",
    "\n",
    "### Comprensión de SparkContext\n",
    "\n",
    "- SparkContext es un punto de entrada al mundo de Spark\n",
    "\n",
    "- Un punto de entrada es una forma de conectarse al clúster Spark\n",
    "\n",
    "- Un punto de entrada es como una llave de la casa \n",
    "\n",
    "- PySpark tiene un SparkContext predeterminado llamado sc\n",
    "\n",
    "### Comandos para inspeccionar SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparContext - punto de entreda para interectuar con Spark\n",
    "\n",
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cargar datos PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5]) \n",
    "\n",
    "rdd2 = sc.textFile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Uso de funciones Lambda, map(), filter()\n",
    "\n",
    "- Son altamente usadas en PySpark\n",
    "\n",
    "- Facilitan la escritura y eficiencia del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "The squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "squared_list_lambda = list(map(lambda x: x **2, my_list))\n",
    "\n",
    "print(\"The squared numbers are\", squared_list_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is: [10, 21, 31, 41, 51, 60, 72, 80, 93, 101]\n",
      "Numbers divisible by 10 are: [10, 60, 80]\n"
     ]
    }
   ],
   "source": [
    "my_list2 = [10,21,31,41,51,60,72,80,93,101]\n",
    "\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programando con Pyspark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyPark SQL y Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning con PySpark MLlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
