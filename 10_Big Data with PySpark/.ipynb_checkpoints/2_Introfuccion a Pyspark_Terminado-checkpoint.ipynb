{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Pyspark\n",
    "\n",
    "1. Conociendo Pyspark\n",
    "\n",
    "2. Manipulando datos\n",
    "\n",
    "3. Comenzando con tuberías de aprendizaje automático\n",
    "\n",
    "4. Modelo, ajuste y selección"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conociendo Pyspark\n",
    "\n",
    "Aprenderá cómo Spark administra los datos y cómo puede leer y escribir tablas desde Python.\n",
    "\n",
    "### ¿Qué es Spark, de todos modos?\n",
    "\n",
    "Spark es una plataforma para la computación en clúster. Spark le permite distribuir datos y cálculos en grupos con múltiples nodos (piense en cada nodo como una computadora separada). Dividir sus datos hace que sea más fácil trabajar con conjuntos de datos muy grandes porque cada nodo solo funciona con una pequeña cantidad de datos.\n",
    "\n",
    "Como cada nodo trabaja en su propio subconjunto de datos totales, también realiza una parte de los cálculos totales requeridos, de modo que tanto el procesamiento de datos como el cómputo se realizan en paralelo sobre los nodos en el clúster. Es un hecho que el cómputo paralelo puede hacer que ciertos tipos de tareas de programación sean mucho más rápidos.\n",
    "\n",
    "Sin embargo, con mayor potencia de cómputo viene una mayor complejidad.\n",
    "\n",
    "Decidir si Spark es o no la mejor solución para su problema requiere algo de experiencia, pero puede considerar preguntas como:\n",
    "\n",
    "¿Mis datos son demasiado grandes para trabajar en una sola máquina?\n",
    "¿Pueden mis cálculos ser paralelizados fácilmente?\n",
    "\n",
    "### Usando Spark en Python\n",
    "\n",
    "El primer paso para usar Spark es conectarse a un clúster.\n",
    "\n",
    "En la práctica, el clúster estará alojado en una máquina remota que está conectada a todos los demás nodos. Habrá una computadora, llamada la maestra, que logra dividir los datos y los cálculos. El maestro está conectado al resto de las computadoras en el clúster, que se denominan trabajador . El maestro envía a los trabajadores datos y cálculos para ejecutar, y ellos envían sus resultados al maestro.\n",
    "\n",
    "Crear la conexión es tan simple como crear una instancia de la SparkContextclase. El constructor de la clase toma algunos argumentos opcionales que le permiten especificar los atributos del clúster al que se está conectando.\n",
    "\n",
    "Se puede crear un objeto que contenga todos estos atributos con el SparkConf() constructor. ¡Eche un vistazo a la documentación para todos los detalles!\n",
    "\n",
    "\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.html\n",
    "\n",
    "### ¿Como nos conectamos a un clúster Spark desde PySpark?\n",
    "\n",
    "Instancia SparkContext clase.\n",
    "\n",
    "Punto de entrada principal para la funcionalidad Spark. Un SparkContext representa la conexión a un clúster de Spark, y puede usarse para crear RDD y emitir variables en ese clúster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comandos utiles\n",
    "\n",
    "SparkContext.version #Versión Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando DataFrames\n",
    "\n",
    "La estructura central de datos de Spark es el conjunto de datos distribuidos resilientes (RDD). Este es un objeto de bajo nivel que le permite a Spark hacer su magia al dividir los datos en múltiples nodos en el clúster. Sin embargo, es difícil trabajar con los RDD directamente, por lo que en este curso utilizará la abstracción Spark DataFrame construida sobre los RDD.\n",
    "\n",
    "Spark DataFrame fue diseñado para comportarse de manera muy similar a una tabla SQL (una tabla con variables en las columnas y observaciones en las filas). No solo son más fáciles de entender, los DataFrames también están más optimizados para operaciones complicadas que los RDD.\n",
    "\n",
    "Cuando comienza a modificar y combinar columnas y filas de datos, hay muchas formas de llegar al mismo resultado, pero algunas a menudo tardan mucho más que otras. Cuando se utilizan RDD, depende del científico de datos encontrar la manera correcta de optimizar la consulta, ¡pero la implementación de DataFrame tiene incorporada gran parte de esta optimización!\n",
    "\n",
    "Para comenzar a trabajar con Spark DataFrames, primero debe crear un SparkSession objeto a partir de su SparkContext. Puede pensar en el SparkContext como su conexión al clúster y SparkSession como su interfaz con esa conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Importación SparkSession\n",
    "\n",
    "my_spark = SparkSession.builder.getOrCreate() #Creando SparkSession\n",
    "\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables() #Ver lista de tablas disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consulta SQL\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "spark.sql(query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.toPandas() #Convirtiendo los datos Spark en dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El createDataFrame() método toma un pandas DataFrame y devuelve un Spark DataFrame.\n",
    "\n",
    "pd_temp = pd.DataFrame(np.random.random(10)) #creamos un dataframe normal\n",
    "\n",
    "spark_temp = spark.createDataFrame(pd_temp) # necesitamos crear un SparkDataFrame con la sesión de spark\n",
    "\n",
    "#La salida de este método se almacena localmente, no en el SparkSession catálogo.\n",
    "\n",
    "#Esto significa que puede usar todos los métodos de Spark DataFrame en él, pero no puede acceder a los datos en otros contextos.\n",
    "\n",
    "#Por ejemplo, una consulta SQL (usando el .sql() método) que hace referencia a su DataFrame arrojará un error.\n",
    "\n",
    "#Para acceder a los datos de esta manera, debe guardarlos como una tabla temporal.\n",
    "\n",
    "#Puede hacerlo utilizando el $.createTempView()$ método Spark DataFrame, que toma como único argumento el nombre de la tabla temporal que desea registrar.\n",
    "\n",
    "#Este método registra el DataFrame como una tabla en el catálogo, pero como esta tabla es temporal, solo se puede acceder desde el específico SparkSession utilizado para crear el Spark DataFrame.\n",
    "\n",
    "print(spark.catalog.listTables()) #Lista de tablas\n",
    "\n",
    "#También está el método createOrReplaceTempView(). \n",
    "\n",
    "#Esto crea de forma segura una nueva tabla temporal si no había nada antes, o actualiza una tabla existente si ya había una definida.\n",
    "\n",
    "spark_temp.createOrReplaceTempView(\"temp\") # Agregagamos el SparkDataFrame al catalogo\n",
    "\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "#Utilizará este método para evitar tener problemas con tablas duplicadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dejando caer al intermediario\n",
    "\n",
    "Ahora ya sabe cómo poner los datos en Spark a través de pandas, pero probablemente se esté preguntando ¿por qué tratar pandas? ¿No sería más fácil leer un archivo de texto directamente en Spark? ¡Por supuesto que sí!\n",
    "\n",
    "Afortunadamente, $SparkSession$ tiene un $.read$ atributo que tiene varios métodos para leer diferentes fuentes de datos en Spark DataFrames.\n",
    "\n",
    "¡Con estos puede crear un DataFrame a partir de un archivo .csv al igual que con pandasDataFrames normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
    "\n",
    "airports = spark.read.csv(file_path, header=True) #carga de datos recordando que spark es una SparkSession\n",
    "\n",
    "airports.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulando datos\n",
    "\n",
    "### Creando columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para sobrescribir una columna existente, simplemente pase el nombre de la columna como primer argumento.\n",
    "\n",
    "#ejemplo 1\n",
    "df = df.withColumn(\"newCol\", df.oldCol + 1)\n",
    "\n",
    "#ejemplo 2\n",
    "flights = spark.table(\"flights\") # Creacion del dataframe catalogo\n",
    "\n",
    "flights = flights.withColumn(\"duration_hrs\", flights.air_time/60) # agregando una nueva columna, duration_hrs\n",
    "\n",
    "### Creando Columnas\n",
    "\n",
    ".withColumn(\"nueva_columna\", nueva_columna)\n",
    "\n",
    ".withColumn(\"nueva_columna\", condicion_booleana)\n",
    "\n",
    ".withColumnRenamed(\"vieja\", \"nueva\") #Cambia el nombre de la columna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrando columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.filter(\"air_time > 120\").show()\n",
    "\n",
    "flights.filter(flights.air_time > 120).show()\n",
    "\n",
    ".filter(\"condicion como en un WHERE\")\n",
    "\n",
    ".filter(condicion booleana) \n",
    "\n",
    ".filter(\"conficion uno\").filter(\"condicion dos\") #Se pueden ir anidando\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccion de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".select(\"columna_a\") #Solo seleccion\n",
    "\n",
    ".select(base.columna_1, base.columna_2)\n",
    "\n",
    "#Pasando un filtro como parametro\n",
    "\n",
    "filtro = (base.columna0 / base.columna1).alias(\"nueva_columna\")\n",
    "\n",
    ".select(\"columna0\", filtro) #pasamos el filtro\n",
    "\n",
    "#como expresion sql podemos colocar el filtro\n",
    "\n",
    ".selectExpr(\"columna0\", \"columna0 / columna1 as nueva_columna\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteo de una columna\n",
    "\n",
    ".groupBy(\"columna\").count().show() \n",
    "\n",
    "#seleccionando la columna de una agrupacion\n",
    "\n",
    ".groupBy().min(\"columna\").show() \n",
    "\n",
    "#filtrando, agrupando y tomando el minimo\n",
    "\n",
    ".filter(columna.a == 'A').groupBy().min(\"columna\").show()\n",
    "\n",
    "\n",
    "#doble filtro, agrupando y tomando el promedio de la columna\n",
    "\n",
    ".filter(columna.a == 'A').filter(columna.b > 10).groupBy().avg(\"columna\").show()\n",
    "\n",
    "#Igual que en groupby podemos agregar funciones\n",
    "\n",
    ".groupBy().agg(F.stddev(\"columna\")).show() \n",
    "\n",
    "\n",
    "#creadno nueva columna y agrupando\n",
    "\n",
    ".withColumn(\"columna\", columna_n + 1 ).groupBy().sum(\"columna_j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uniones\n",
    "\n",
    "tabla_A.join(tabla_B, on = \"llave\", how = \"leftouter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "En el núcleo del pyspark.ml módulo están las clases Transformer y Estimator.\n",
    "\n",
    "Casi todas las demás clases del módulo se comportan de manera similar a estas dos clases básicas.\n",
    "\n",
    "Transformer las clases tienen un .transform() método que toma un DataFrame y devuelve un nuevo DataFrame; generalmente el original con una nueva columna adjunta. \n",
    "\n",
    "Por ejemplo, puede usar la clase Bucketizer para crear contenedores discretos a partir de una entidad continua o la clase PCA para reducir la dimensionalidad de su conjunto de datos mediante el análisis de componentes principales.\n",
    "\n",
    "Estimator todas las clases implementan un .fit() método.\n",
    "\n",
    "Estos métodos también toman un DataFrame, pero en lugar de devolver otro DataFrame, devuelven un objeto modelo. \n",
    "\n",
    "Esto puede ser algo así como StringIndexerModel para incluir datos categóricos guardados como cadenas en sus modelos, o RandomForestModel que utiliza el algoritmo de bosque aleatorio para clasificación o regresión.\n",
    "\n",
    " - Tranformer: clases tienen método .transform() toma un DataFrame y devuelve nuevo Data Frame.\n",
    "\n",
    " - Estimator: clases tienen método .fit() toma un DataFrame y devuelve un objeto modelo.\n",
    " \n",
    " - Tipos de datos númericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devuelve un nuevo dataframe\n",
    ".transform() \n",
    "\n",
    "#Objeto modelo\n",
    ".fit() \n",
    "\n",
    "#Tipos de datos para los algoritmos en Spark deben ser númericos.\n",
    "dataframe = dataframe.withColumn(\"col\", dataframe.columna.cast(\"tipo-numerico\")) #cambio de tipo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Texto a valores númericos?\n",
    "\n",
    "PySpark tiene funciones para manejar esto en pyspark.ml.features.\n",
    "\n",
    "Primero codificamos la característica categórica StringIndexer. Asigna a cada cadena un número y luego volvemos en formato de columna.\n",
    "\n",
    "Segundo codificamos esta columna númerica como un vector OneHotEncoder.\n",
    "\n",
    "Tercer ensamblar todo en un vector con todas las variables númericas que usaremos.\n",
    "\n",
    "Teniendo como resultado final una columna codificada de las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero StringIndexer, inputCol nombre de la columna que deseamos indexar y outpuCol nombre de la nueva columna.\n",
    "\n",
    "#Paso 1\n",
    "\n",
    "carr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\") # Create a StringIndexer\n",
    "\n",
    "dest_indexer = StringIndexer(inputCol=\"dest\",outputCol=\"dest_index\") # Create a StringIndexer\n",
    "\n",
    "#Paso 2\n",
    "\n",
    "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\") # Create a OneHotEncoder\n",
    "\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_index\", outputCol=\"dest_fact\") # Create a OneHotEncoder\n",
    "\n",
    "#Pas 3 \n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"month\",\"air_time\",\"carrier_fact\",\"dest_fact\",\"plane_age\"], outputCol=\"features\") # Ensamblador de un vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente se esta listo para crear un Pipeline\n",
    "\n",
    "Pipeline es una clase en el pyspark.ml módulo que combina toda la Estimators y Transformers que ya ha creado.\n",
    "\n",
    "Esto le permite reutilizar el mismo proceso de modelado una y otra vez envolviéndolo en un objeto simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Pipeline\n",
    "flights_pipe = Pipeline(stages = [dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos Test vs Train\n",
    "\n",
    "Después de limpiar sus datos y prepararlos para el modelado, uno de los pasos más importantes es dividir los datos en un conjunto de prueba y un conjunto de trenes.\n",
    "\n",
    "Después de eso, ¡no toque los datos de su prueba hasta que crea que tiene un buen modelo!\n",
    "\n",
    "A medida que construye modelos y forma hipótesis, puede probarlos en sus datos de entrenamiento para tener una idea de su rendimiento.\n",
    "\n",
    "### Transformando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_data = flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir antes de modelar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = piped_data.randomSplit([.6, .4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de ajuste y selección\n",
    "\n",
    "Una ves limpios los datos y con la estrectura adecuada para correr modelos con Spark, veamos como ajustar un modelo con PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación cruzada\n",
    "\n",
    "Ajustará su modelo de regresión logística mediante un procedimiento llamado validación cruzada k-fold. \n",
    "\n",
    "Este es un método para estimar el rendimiento del modelo en datos no vistos (como su testDataFrame).\n",
    "\n",
    "Funciona dividiendo los datos de entrenamiento en algunas particiones diferentes.\n",
    "\n",
    "El número exacto depende de usted. Una vez que los datos se dividen, una de las particiones se reserva y el modelo se ajusta a los demás.\n",
    "\n",
    "Luego, el error se mide contra la partición retenida.\n",
    "\n",
    "Esto se repite para cada una de las particiones, de modo que cada bloque de datos se mantiene y se usa como un conjunto de prueba exactamente una vez.\n",
    "\n",
    "Luego se promedia el error en cada una de las particiones. Esto se denomina error de validación cruzada del modelo y es una buena estimación del error real en los datos retenidos.\n",
    "\n",
    "¡Utilizará la validación cruzada para elegir los hiperparámetros creando una cuadrícula de los posibles pares de valores para los dos hiperparámetros elasticNetParam y regParam, y utilizando el error de validación cruzada para comparar todos los diferentes modelos para que pueda elegir el mejor!\n",
    "\n",
    "La validación cruzada nos permite medir el error de validación en el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el evaluador\n",
    "\n",
    "Lo primero que necesita al realizar la validación cruzada para la selección del modelo es una forma de comparar diferentes modelos.\n",
    "\n",
    "Afortunadamente, el pyspark.ml.evaluation submódulo tiene clases para evaluar diferentes tipos de modelos.\n",
    "\n",
    "Su modelo es un modelo de clasificación binaria, por lo que utilizará el BinaryClassificationEvaluator del pyspark.ml.evaluation módulo.\n",
    "\n",
    "Este evaluador calcula el área bajo el ROC.\n",
    "\n",
    "Esta es una métrica que combina los dos tipos de errores que puede hacer un clasificador binario (falsos positivos y falsos negativos) en un número simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación del submodulo\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Creación de BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacer una cuadrícula\n",
    "\n",
    "A continuación, debe crear una cuadrícula de valores para buscar cuando busque los hiperparámetros óptimos.\n",
    "\n",
    "El submódulo pyspark.ml.tuningin cluye una clase llamada ParamGridBuilder que hace exactamente eso (¡tal vez estás comenzando a notar un patrón aquí; PySpark tiene un submódulo para casi todo!).\n",
    "\n",
    "Deberá usar los métodos .addGrid() y .build() para crear una cuadrícula que pueda usar para la validación cruzada.\n",
    "\n",
    "El .addGrid() método toma un parámetro de modelo (un atributo del modelo Estimator, lr, que creó hace unos ejercicios) y una lista de valores que desea probar.\n",
    "\n",
    "El .build() método no toma argumentos, solo devuelve la cuadrícula que usará más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación del submodulo\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "# Cuadrícula de valores\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Agregamos hiperparametros\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# Cuadricula para usar valores\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacer el validador\n",
    "\n",
    "Con lo anterior definido, lr (modelo), grid (cuadricula con hiperparametros) y evaluator (evaluador del modelo en este caso curva ROC)\n",
    "\n",
    "El submódulo pyspark.ml.tuning también tiene una clase llamada CrossValidator para realizar validación cruzada.\n",
    "\n",
    "Esto Estimator toma el modelador que desea ajustar, la cuadrícula de hiperparámetros que creó y el evaluador que desea usar para comparar sus modelos.\n",
    "\n",
    "Creará al CrossValidator pasarle la regresión logística Estimator lr, el parámetro grid y el evaluator que se creó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del Crossvalidator\n",
    "\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "                         estimatorParamMaps=grid,\n",
    "                         evaluator=evaluator\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustar el modelo (s)\n",
    "\n",
    "¡Finalmente está listo para adaptarse a los modelos y seleccionar el mejor!\n",
    "\n",
    "Desafortunadamente, la validación cruzada es un procedimiento computacionalmente intensivo.\n",
    "\n",
    "Ajustar todos los modelos llevaría demasiado tiempo, para hacer esto localmente, usaría el código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasando los datos de entrenamiento a la validación cruzada\n",
    "models = cv.fit(training)\n",
    "\n",
    "# extracción del mejor modelo\n",
    "best_lr = models.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerde, se llaman los datos de entrenamiento training y los está utilizando lr para ajustar un modelo de regresión logística.\n",
    "\n",
    "La validación cruzada seleccionó los valores de los parámetros regParam=0 y elasticNetParam=0 como los mejores.\n",
    "\n",
    "Estos son los valores predeterminados, por lo que no necesita hacer nada más lrantes de ajustar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamar lr.fit()\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "# imprime el mejor best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una métrica común para los algoritmos de clasificación binaria, llamada AUC , o área bajo la curva.\n",
    "\n",
    "En este caso, la curva es el ROC, o la curva de operación del receptor.\n",
    "\n",
    "¡Todo lo que necesita saber es que para nuestros propósitos, cuanto más cerca esté el AUC de uno (1), mejor será el modelo!\n",
    "\n",
    "Si ha creado un modelo de clasificación binaria perfecto, ¿cuál sería el AUC? 1\n",
    "\n",
    "### Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando el modelo con los datos test\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# evaluando\n",
    "print(evaluator.evaluate(test_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
